{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding:utf-8 -*- \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_chromsome_fragment(data, input_filename, validate_filename):\n",
    "    reverse_it = {'A':'T','C':'G', \"T\":\"A\", \"G\":\"C\", 'a':'t','c':'g', \"t\":\"a\", \"g\":\"c\"}\n",
    "\n",
    "    for each_row in data.get_values():\n",
    "        each_row = list(each_row)\n",
    "        chromosome_id = each_row[0].strip()\n",
    "        geneID = each_row[9].split(\"=\")[-1]\n",
    "        start = int( each_row[2] )\n",
    "        end = int( each_row[3] )\n",
    "        direction = each_row[4]\n",
    "\n",
    "#         print(each_row)\n",
    "        \n",
    "        try :\n",
    "#             with open(\"ysxhg19.fa\") as f:\n",
    "            with open(input_filename) as f:\n",
    "                found = False\n",
    "                while (True):\n",
    "                    each_line = f.readline()\n",
    "                    if each_line == \"\": break\n",
    "                    each_line = each_line.strip()\n",
    "#                     print('start',len(each_line),\">=5\",each_line[:4],\"=\" , \">chr\",each_line[4:],\"=\" , chromosome_id)\n",
    "                    if 'chr' in each_line and each_line.split(\"chr\")[-1].strip() == chromosome_id:\n",
    "                        print(each_line.split(\"chr\"))\n",
    "                        found = True\n",
    "                        first_line = f.readline()\n",
    "                        first_line = first_line.strip()\n",
    "                        line_len = len(first_line)\n",
    "\n",
    "                        start_line = start // line_len + ((start%line_len) > 0) # 假设起始行是1行\n",
    "                        end_line = end // line_len + ((end%line_len) > 0)\n",
    "\n",
    "                        start_index = start%line_len\n",
    "                        if start_index == 0:\n",
    "                            start_index = line_len\n",
    "                        start_index -= 1 # 转换为以0开始的索引\n",
    "\n",
    "                        number = end - start + 1\n",
    "\n",
    "                        fragment = \"\"\n",
    "                        line_index = 1\n",
    "                        if line_index >= start_line:\n",
    "#                             print(\"index\",line_index)\n",
    "                            fragment += first_line\n",
    "                        \n",
    "                        for each_new_line in f:\n",
    "                           if 'chr' in each_line and each_line.split(\"chr\")[-1].strip() != chromosome_id:\n",
    "#                                print(\"the range fragment is out of range\")\n",
    "                                print(\"起始位或终止位超出范围了\")\n",
    "                \n",
    "                            line_index += 1\n",
    "                            if line_index < start_line:\n",
    "                                continue\n",
    "                            if line_index > end_line:\n",
    "                                break\n",
    "                            \n",
    "#                             print(\"index\",line_index)\n",
    "                            fragment += each_new_line.strip() \n",
    "                            \n",
    "#                         print('origin')\n",
    "#                         print(fragment)\n",
    "#                         print('uuuuuuuuuuuuuuuuu')\n",
    "                        fragment = fragment[start_index:start_index+number]\n",
    "                        fragment2 = fragment\n",
    "\n",
    "                        \"\"\"反向互补配对\"\"\"\n",
    "                        if direction == '-':\n",
    "                            new_fragment = \"\"\n",
    "                            for b in fragment:\n",
    "                                b = reverse_it.get(b,b)\n",
    "                                new_fragment = b + new_fragment\n",
    "                            fragment = new_fragment\n",
    "\n",
    "                        print(fragment)\n",
    "\n",
    "                        if fragment != \"\":\n",
    "                            with open(\"result.tmp\", \"a\") as result_file:\n",
    "#                                 print(gene_id, fragment, file=result_file)\n",
    "                                result_file.write(\" \".join([str(each) for each in [gene_id, fragment]]) + \"\\n\")\n",
    "                            \n",
    "                            with open(\"result2.tmp\", \"a\") as result_file2:\n",
    "#                                 print(start, end, chromosome_id, direction, fragment, file=result_file2)\n",
    "                                result_file2.write(\" \".join([str(each) for each in [start, end, chromosome_id, direction, fragment]]) + \"\\n\")\n",
    "                        \n",
    "#                         with open(\"validate.tmp\", \"a\") as validate_file:\n",
    "                        with open(validate_filename, \"a\") as validate_file:\n",
    "                            if fragment == \"\":\n",
    "#                                 print(\"may occur error\", file=validate_file)\n",
    "                                validate_file.write(\"may occur error\\n\")\n",
    "#                                 print(start,end,start_line,end_line,start_index, number, fragment2, file=validate_file)\n",
    "                                validate_file.write(\" \".join([str(each)for each in [start,end,start_line,end_line,start_index, number, fragment2]]) + \"\\n\")\n",
    "#                                 print(\"---\", file=validate_file)\n",
    "                                validate_file.write(\"---\\n\")\n",
    "                            else:\n",
    "#                                 print(*each_row, end=\" \", file = validate_file)\n",
    "                                validate_file.write(\" \".join([str(each) for each in each_row]) + \" \")\n",
    "#                                 print(fragment, file = validate_file)\n",
    "                                validate_file.write(str(fragment) + \"\\n\")\n",
    "#                                 print(start,end,start_line,end_line,start_index, number,fragment2, fragment, file=validate_file)\n",
    "                                validate_file.write(\" \".join([str(each) for each in [start,end,start_line,end_line,start_index, number,fragment2, fragment]]) + \"\\n\")\n",
    "                        break\n",
    "                if found == False:\n",
    "                    print(\"chromosome\" , chromosome_id, \"not found\")\n",
    "        except EOFError:\n",
    "            print(\"eroor\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询染色体片段\n",
      "输入基因文件路径，如ysxgff.gff\n",
      "ysxgff.gff\n",
      "输入染色体文件路径，如ysxhg19.fa\n",
      "chr11.fa\n",
      "输入临时文件路径\n",
      "3\n",
      "输入gene_id（q退出程序）\n",
      "ENSG00000226743\n",
      "['>', '11']\n",
      "GCTCCCGCCTGCGTTTGTAGACCCCAAAGTTTCTGCAACCAAGCTCTTCAGACCCACATCCCTTCTCCCAGT\n",
      "['>', '11']\n",
      "NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n",
      "['>', '11']\n",
      "AGGAGCTGGGCCGCACGCGGGCTGCTGGGAGGCAGGCAGGGACTTGGCCCCGAGAGGCCGCCGTGGGGG\n",
      "['>', '11']\n",
      "AGCTGGGCCTGGAGAGGCCCCTGGGAGGCAAGGGCGGGGCCTGCAGAGGCTGTTCTCCAACCAGTGCTAGAACTGTAC\n",
      "['>', '11']\n",
      "CACCAGGAGGCAGGAGGTGGGCCCTCAGAGCTTGGCTGGAGAAAGTTCGGGGCCTACAAAG\n",
      "['>', '11']\n",
      "CTGGGCAGGAGTTGAGCCAAAAGAGCTTG\n",
      "q\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "print(\"查询染色体片段\")\n",
    "gene_input_filename = input(\"输入基因文件路径，如ysxgff.gff\\n\")\n",
    "chromsome_input_filename = input(\"输入染色体文件路径，如ysxhg19.fa\\n\")\n",
    "validate_filename = input(\"输入临时文件路径\\n\")\n",
    "print(\"输入gene_id（q退出程序）\")\n",
    "\n",
    "\"\"\"\n",
    "python 2.x\n",
    "print(\"查询染色体片段\".decode('utf-8').encode('gbk'))\n",
    "gene_input_filename = raw_input(\"输入基因文件路径，如ysxgff.gff\\n\".decode('utf-8').encode('gbk'))\n",
    "chromsome_input_filename = raw_input(\"输入染色体文件路径，如ysxhg19.fa\\n\".decode('utf-8').encode('gbk'))\n",
    "validate_filename = raw_input(\"输入临时文件路径\\n\".decode('utf-8').encode('gbk'))\n",
    "print(\"输入gene_id（q退出程序）\".decode('utf-8').encode('gbk'))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "while(True):\n",
    "# \\s*正则识别多个空格\n",
    "#     gd = pd.read_csv('ysxgff.gff',sep=\"\\s*\", header = None, chunksize=10000, encoding = 'gbk', engine='python')\n",
    "    gd = pd.read_csv(gene_input_filename,sep=\"\\s*\", header = None, chunksize=10000, encoding = 'gbk', engine='python')\n",
    "    \n",
    "    gene_id = \"1\"\n",
    "    gene_id = str(input())\n",
    "    \n",
    "    if gene_id.lower() == \"q\":\n",
    "        print(\"quit\")\n",
    "        break\n",
    "    \n",
    "    cnt = 0\n",
    "    while(True):\n",
    "        try:\n",
    "            \n",
    "            gdc = gd.get_chunk()\n",
    "            gdc = gdc.astype(str)\n",
    "            gdc2 = gdc.copy()\n",
    "            gdc2.ix[:, 6] = gdc2.ix[:, 6].apply(lambda x: x.split(\"=\")[-1])\n",
    "            gdc2.ix[:, 7] = gdc2.ix[:, 7].apply(lambda x: x.split(\"=\")[-1])\n",
    "            gdc2.ix[:, 8] = gdc2.ix[:, 8].apply(lambda x: x.split(\"=\")[-1])\n",
    "            gdc2.ix[:, 9] = gdc2.ix[:, 9].apply(lambda x: x.split(\"=\")[-1])\n",
    "            \n",
    "            \n",
    "#             print( gdc2.apply(lambda x: x[9] == gene_id and x[1] == \"CDS\" , axis=1) )\n",
    "            \n",
    "            gdc = gdc[ gdc2.apply(lambda x: x[9] == gene_id and x[1] == \"CDS\" , axis=1) ]\n",
    "#            if len(gdc) > 0:\n",
    "#                print(\"len\",len(gdc))\n",
    "            \n",
    "#             print(\"bool=>\\n\", gdc2[9]==gene_id ,'\\n---\\n', gdc2[1]==\"CDS\", \"end\")\n",
    "            \n",
    "#             print(\"select=>\\n\", gdc[gdc2[9]==gene_id])\n",
    "            \n",
    "#             print(cnt,\"=>\\n\",gdc2)\n",
    "            \n",
    "#             gdc = gdc[gdc2[9]==gene_id]\n",
    "            \n",
    "            cnt += len(gdc)\n",
    "                \n",
    "#             print(cnt,\"=>\\n\",gdc)\n",
    "            \n",
    "            if len(gdc) > 0:\n",
    "                get_chromsome_fragment(gdc, chromsome_input_filename, validate_filename)\n",
    "            \n",
    "        except StopIteration:\n",
    "            if cnt == 0:\n",
    "                print(\"gene_id notFound\")\n",
    "                with open(\"result.tmp\", \"a\") as result_file:\n",
    "#                     print(gene_id, \"notFound\", file=result_file)\n",
    "                    result_file.write(str(gene_id) + \"notFound\\n\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers:\n",
      "\n",
      "read_csv(filepath_or_buffer, sep=',', dialect=None, compression='infer', doublequote=True, escapechar=None, quotechar='\"', quoting=0, skipinitialspace=False, lineterminator=None, header='infer', index_col=None, names=None, prefix=None, skiprows=None, skipfooter=None, skip_footer=0, na_values=None, true_values=None, false_values=None, delimiter=None, converters=None, dtype=None, usecols=None, engine=None, delim_whitespace=False, as_recarray=False, na_filter=True, compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, warn_bad_lines=True, error_bad_lines=True, keep_default_na=True, thousands=None, comment=None, decimal=b'.', parse_dates=False, keep_date_col=False, dayfirst=False, date_parser=None, memory_map=False, float_precision=None, nrows=None, iterator=False, chunksize=None, verbose=False, encoding=None, squeeze=False, mangle_dupe_cols=True, tupleize_cols=False, infer_datetime_format=False, skip_blank_lines=True)\n",
      "    Read CSV (comma-separated) file into DataFrame\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the `online docs for IO Tools\n",
      "    <http://pandas.pydata.org/pandas-docs/stable/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : string or file handle / StringIO\n",
      "        The string could be a URL. Valid URL schemes include\n",
      "        http, ftp, s3, and file. For file URLs, a\n",
      "        host is expected. For instance, a local file could be\n",
      "        file ://localhost/path/to/table.csv\n",
      "    sep : string, default ','\n",
      "        Delimiter to use. If sep is None, will try to automatically determine\n",
      "        this. Regular expressions are accepted.\n",
      "    engine : {'c', 'python'}\n",
      "        Parser engine to use. The C engine is faster while the python engine is\n",
      "        currently more feature-complete.\n",
      "    lineterminator : string (length 1), default None\n",
      "        Character to break file into lines. Only valid with C parser\n",
      "    quotechar : string (length 1)\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default None\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "        Default (None) results in QUOTE_MINIMAL behavior.\n",
      "    skipinitialspace : boolean, default False\n",
      "        Skip spaces after delimiter\n",
      "    escapechar : string (length 1), default None\n",
      "        One-character string used to escape delimiter when quoting is QUOTE_NONE.\n",
      "    dtype : Type name or dict of column -> type, default None\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n",
      "        (Unsupported with engine='python')\n",
      "    compression : {'gzip', 'bz2', 'infer', None}, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer', then use gzip or\n",
      "        bz2 if filepath_or_buffer is a string ending in '.gz' or '.bz2',\n",
      "        respectively, and no decompression otherwise. Set to None for no\n",
      "        decompression.\n",
      "    dialect : string or csv.Dialect instance, default None\n",
      "        If None defaults to Excel dialect. Ignored if sep longer than 1 char\n",
      "        See csv.Dialect documentation for more details\n",
      "    header : int, list of ints, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Defaults to 0 if no ``names`` passed, otherwise ``None``. Explicitly\n",
      "        pass ``header=0`` to be able to replace existing names. The header can be\n",
      "        a list of integers that specify row locations for a multi-index on the\n",
      "        columns E.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example are skipped). Note that this parameter\n",
      "        ignores commented lines and empty lines if ``skip_blank_lines=True``, so header=0\n",
      "        denotes the first line of data rather than the first line of the file.\n",
      "    skiprows : list-like or integer, default None\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file\n",
      "    index_col : int or sequence or False, default None\n",
      "        Column to use as the row labels of the DataFrame. If a sequence is given, a\n",
      "        MultiIndex is used. If you have a malformed file with delimiters at the end\n",
      "        of each line, you might consider index_col=False to force pandas to _not_\n",
      "        use the first column as the index (row names)\n",
      "    names : array-like, default None\n",
      "        List of column names to use. If file contains no header row, then you\n",
      "        should explicitly pass header=None\n",
      "    prefix : string, default None\n",
      "        Prefix to add to column numbers when no header, e.g 'X' for X0, X1, ...\n",
      "    na_values : str, list-like or dict, default None\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values\n",
      "    true_values : list, default None\n",
      "        Values to consider as True\n",
      "    false_values : list, default None\n",
      "        Values to consider as False\n",
      "    keep_default_na : bool, default True\n",
      "        If na_values are specified and keep_default_na is False the default NaN\n",
      "        values are overridden, otherwise they're appended to\n",
      "    parse_dates : boolean, list of ints or names, list of lists, or dict, default False\n",
      "        If True -> try parsing the index.\n",
      "        If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.\n",
      "        If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.\n",
      "        {'foo' : [1, 3]} -> parse columns 1, 3 as date and call result 'foo'\n",
      "        A fast-path exists for iso8601-formatted dates.\n",
      "    keep_date_col : boolean, default False\n",
      "        If True and parse_dates specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, default None\n",
      "        Function to use for converting a sequence of string columns to an\n",
      "        array of datetime instances. The default uses dateutil.parser.parser\n",
      "        to do the conversion. Pandas will try to call date_parser in three different\n",
      "        ways, advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by parse_dates) as arguments; 2) concatenate (row-wise) the string\n",
      "        values from the columns defined by parse_dates into a single array and pass\n",
      "        that; and 3) call date_parser once for each row using one or more strings\n",
      "        (corresponding to the columns defined by parse_dates) as arguments.\n",
      "    dayfirst : boolean, default False\n",
      "        DD/MM format dates, international and European format\n",
      "    thousands : str, default None\n",
      "        Thousands separator\n",
      "    comment : str, default None\n",
      "        Indicates remainder of line should not be parsed. If found at the\n",
      "        beginning of a line, the line will be ignored altogether. This parameter\n",
      "        must be a single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header`\n",
      "        but not by `skiprows`. For example, if comment='#', parsing\n",
      "        '#empty\\na,b,c\\n1,2,3' with `header=0` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point. E.g. use ',' for European data\n",
      "    nrows : int, default None\n",
      "        Number of rows of file to read. Useful for reading pieces of large files\n",
      "    iterator : boolean, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with ``get_chunk()``.\n",
      "    chunksize : int, default None\n",
      "        Return TextFileReader object for iteration. `See IO Tools docs for more\n",
      "        information\n",
      "        <http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_ on\n",
      "        ``iterator`` and ``chunksize``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c')\n",
      "    converters : dict, default None\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels\n",
      "    verbose : boolean, default False\n",
      "        Indicate number of NA values placed in non-numeric columns\n",
      "    delimiter : string, default None\n",
      "        Alternative argument name for sep. Regular expressions are accepted.\n",
      "    encoding : string, default None\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_\n",
      "    squeeze : boolean, default False\n",
      "        If the parsed data only contains one column then return a Series\n",
      "    na_filter : boolean, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file\n",
      "    usecols : array-like, default None\n",
      "        Return a subset of the columns.\n",
      "        Results in much faster parsing time and lower memory usage.\n",
      "    mangle_dupe_cols : boolean, default True\n",
      "        Duplicate columns will be specified as 'X.0'...'X.N', rather than 'X'...'X'\n",
      "    tupleize_cols : boolean, default False\n",
      "        Leave a list of tuples on columns as is (default is to convert to\n",
      "        a Multi Index on the columns)\n",
      "    error_bad_lines : boolean, default True\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will dropped from the DataFrame that is\n",
      "        returned. (Only valid with C parser)\n",
      "    warn_bad_lines : boolean, default True\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output. (Only valid with C parser).\n",
      "    infer_datetime_format : boolean, default False\n",
      "        If True and parse_dates is enabled for a column, attempt to infer\n",
      "        the datetime format to speed up the processing\n",
      "    skip_blank_lines : boolean, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    result : DataFrame or TextParser\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python34\\lib\\site-packages\\ipykernel\\__main__.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators; you can avoid this warning by specifying engine='python'.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "gd = pd.read_csv('test',sep=\"\\s*\", header = None, chunksize=10000, encoding = 'gbk')\n",
    "a = gd.get_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = a.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  12  55\n",
       "1  12   3\n",
       "2  12   t"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'55'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.ix[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = open(\"result.tmp\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print (s.readline()==\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
